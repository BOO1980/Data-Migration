<!DOCTYPE html>
<!--
To change this license header, choose License Headers in Project Properties.
To change this template file, choose Tools | Templates
and open the template in the editor.
-->
<html>
    <head>
        <title>Data Quality</title>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    </head>
    <body>
        <h1>Data Quality</h1>
        <ul><li>Businesses spend billions of dollars migrating data between information-intensive applications</li>
            <li>75% of new systems fail to meet expectations, often because flaws in the migration process result in data that has not be validated for the intended task</li>
            <li>Because the systems itself is seen as the investment, any data migration effort is often viewed as necessary but unfortunate cost, leading to an oversimplified under funded approach</li>
            <li>With an understanding of the hidden challenges, managing the migration part as part of the investments is more likely to deliver accurate data.  Delivery accurate data mitigates the risk of:
                <ul><li>Delays</li>
                    <li>Budget overruns</li>
                    <li>Scope reductions</li></ul></li>
            <li>Data migration generally results from the introduction of a new system
                <ul>
                    <li>This may involve an application migration or consolidation in which one or more legacy systems are replaced</li>
                    <li>This may involve an additional system that will sit alongside the existing applications</li>
                </ul>                 
            </li>
            <li>The ultimate aim is to improve corporate performance and deliver competitive advantage</li>
            <li>Accurate data is the raw material that maximises the value of enterprise applications</li>
            <li>When existing data is migrated to a new target application it can contain:
                <ul>
                    <li>inaccuracies</li>
                    <li>unknowns</li>
                    <li>redundant material</li>
                    <li>duplicate material</li>
                </ul>
            </li>
            <li>Even though the data may be perfectly adequate for its current use, it may be wholly inadequate in terms of content and structure for the objectives of the target system</li>
            <li>Without understanding of both source and target, transferring data into a more sophisticated application will amplify the negative impacts such as:
                <ul><li>Incorrect data</li>
                    <li>Irrelevant data</li>
                    <li>Perpetuate hidden legacy problems</li>
                    <li>Increase exposure to risk</li></ul>
            <li>Data migration is usually part of a larger project deliverable</li>
            <li>Generally the majority of the business attention is focused on the package selection and configuration rather than on ensuring that the data that populates the new system is fit for purpose</li>
            <li>Data migration planning is seen as a simple matter of shifting data from one bucket to another</li>
            <li>Organisations planning a data migration should consider which style of migrations is most suitable to their needs</li>
            <li>Organisations can choose from several strategies depending on the project requirements and available processing windows</li>
            <li>There are two principal types of migration: 
                <ul>
                    <li>Big Bang Migration
                        <ul>
                            <li>Completing the entire migration in a small defined processing window.</li>
                            <li>System downtime while the data is extracted from the source systems, processed and loaded to the target. </li>
                            <li>Switching of processing over to the new environment</li>
                            <li>Completes the migration in the shortest-possible time</li>
                            <li>Can carry several risks</li>
                            <li>Few organisations can live with a core system being unavailable for so long</li>
                            <li>Businesses adopting this approach should do at least one dry run</li>
                        </ul>
                    </li>
                    <li>Trickle Migrations<ul>
                            <li>Trickle migrations take an incremental approach to migrating data</li>
                            <li>A trickle migration involves running the old and new systems in parallel and migrating the data in phases</li>
                            <li>The method inherently provides the zero downtime that mission-critical applications requiring 24/7 operation need</li>
                            <li>A trickle migration can be implemented with real-time processes to move data, and these processes can also be used to maintain the data by passing future changes to the target system</li>
                            <li>The trickle approach can add some complexity to the design</li> 
                            <li>It must be possible to track which data has been migrated</li>
                            <li>If this is part of a systems migration it may also mean that source and target systems run parallel with users having to switch between them</li>
                            <li>Alternatively the old system(s) can continue to be operational until the entire migration is completed, before users are switched to the new system</li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>The initial data migration is not the end of the matter</li>
            <li>Source and target systems will need to coexist and the congruence of their data needs to be maintained</li>
            <li>If the target system is purely analytical it may be possible to satisfy this requirement by rerunning the entire data migration periodically</li>
            <li>If changes may be made to the data in both the source and target systems, synchronising them can be more complex</li>
        </ul>
        
        <h2>Key drivers of data complexity</h2>
        <p>A combination of trends is accelerating the need to manage data migration activity more effectively as part of a corporate data quality strategy
        <dl><dt>Corporate growth</dt>
            <dd>Mergers, acquisitions and restructuring of disparate systems create new data and new data sources</dd>
            <dt>Compliance</dt><dd>Data must be validated against regulations and standards</dd>
            <dt>Data Volume</dt><dd>escalating amounts of data are increasing the burden of data management</dd>
            <dt>Data diversity</dt><dd>Introduction of data in new formats</dd>
            <dt>Data Decay</dt><dd>Data is volatile, customer data typically deteriorates at a rate of 10 percent to 25 percent per year</dd>
            <dt>Data Denial</dt><dd>Organisation are often unaware of their data quality issues and lack the expertise of a senior sponsor to champion decisive action</dd>
            <dt>Technical advances</dt><dd>Proliferation of new devices, platforms, and operating systems also contributes to complexity</dd>
            <dt>Economic factors</dt><dd>With  pressure on margins, all corporate data must help organisations compete more effectively</dd>
        </dl>  
    </body>
</html>
